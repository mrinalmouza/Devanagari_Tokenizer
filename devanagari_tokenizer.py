# -*- coding: utf-8 -*-
"""Devanagari_tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1swWuWnwQEfg5tSQcj-O9Gc-EMsraGq6-
"""

import os
from collections import Counter
import multiprocessing
import json
import regex as re
import json


class DevanagariTokenizer:
    def __init__(self, merges_path):
        with open(merges_path, 'r') as f:
            merges = json.load(f)
        
        self.merges = {}
        for k, v in merges.items():
            try:
                tuple_value = tuple(map(int, k.split(',')))
                self.merges[tuple_value] = v
            except:
                print("Key:", k)
                print("Value:", v)
        
        # Initialize vocabulary
        self.vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]

    def _clean_text(self, text: str) -> str:
        """
        Clean text to keep ONLY Hindi characters.
        Removes:
        - Emojis
        - English characters
        - Other scripts
        - Punctuation
        - Extra whitespace
        """

        # Step 1: Remove emojis
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "]+", flags=re.UNICODE)
        text = emoji_pattern.sub('', text)

        # Step 2: Remove URLs
        text = re.sub(r'http\S+|www.\S+', '', text)

        # Step 3: Keep ONLY Devanagari and spaces
        # \u0900-\u097F is Devanagari range
        allowed_pattern = re.compile(r'[^\u0900-\u097F\s]')
        text = allowed_pattern.sub('', text)

        # Step 4: Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def _get_pairs(self, tokens):
        return Counter(zip(tokens, tokens[1:]))

    def _merge(self, input_chars, pair, new_token):
        new_tokens = []
        #print(len(input_chars))
        i = 0
        while i < len(input_chars):
            if i < len(input_chars)-1 and input_chars[i] == pair[0] and input_chars[i+1] == pair[1]:
                new_tokens.append(new_token)
                i += 2
            else:
                new_tokens.append(input_chars[i])
                i += 1
        return new_tokens

    def encode(self, input_text):
        cleaned_text = self._clean_text(input_text)
        tokens = list(cleaned_text.encode("utf-8"))
        while True:
            stats = self._get_pairs(tokens)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            if pair not in self.merges:
                break  # Nothing else can be merged
            idx = self.merges[pair]
            tokens = self._merge(tokens, pair, idx)
        return tokens

    def decode(self, ids):
        tokens = b"".join(self.vocab[idx] for idx in ids)
        text = tokens.decode("utf-8", errors="replace")
        return text


